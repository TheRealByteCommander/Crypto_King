# Ollama LLM Setup f√ºr Project CypherTrade

Dieses Projekt ist f√ºr die Verwendung mit **Ollama** (lokale LLMs) konfiguriert.

## üöÄ Ollama Installation

### Linux / macOS

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### Windows

Download von: https://ollama.com/download

## üì¶ Modelle herunterladen

### Empfohlene Modelle f√ºr Trading:

**Llama 3.2 (Standard, empfohlen):**
```bash
ollama pull llama3.2
```

**Alternative Modelle:**

```bash
# Llama 3.1 - Gr√∂√üeres Modell, bessere Reasoning
ollama pull llama3.1

# Mistral - Gut f√ºr strukturierte Ausgaben
ollama pull mistral

# Gemma 2 - Schnell und effizient
ollama pull gemma2

# DeepSeek Coder - Spezialisiert auf Code/Daten
ollama pull deepseek-coder
```

## ‚öôÔ∏è Konfiguration

### 1. Ollama Server starten

```bash
ollama serve
```

Der Server l√§uft auf: `http://localhost:11434`

### 2. Backend .env anpassen

Die `/app/backend/.env` ist bereits f√ºr Ollama konfiguriert:

```env
# Ollama Base URL
OLLAMA_BASE_URL="http://localhost:11434/v1"
OLLAMA_API_KEY="ollama"

# NexusChat Agent
NEXUSCHAT_LLM_PROVIDER="ollama"
NEXUSCHAT_MODEL="llama3.2"
NEXUSCHAT_BASE_URL="http://localhost:11434/v1"

# CypherMind Agent
CYPHERMIND_LLM_PROVIDER="ollama"
CYPHERMIND_MODEL="llama3.2"
CYPHERMIND_BASE_URL="http://localhost:11434/v1"

# CypherTrade Agent
CYPHERTRADE_LLM_PROVIDER="ollama"
CYPHERTRADE_MODEL="llama3.2"
CYPHERTRADE_BASE_URL="http://localhost:11434/v1"
```

### 3. Verschiedene Modelle pro Agent verwenden

Sie k√∂nnen f√ºr jeden Agent ein unterschiedliches Modell verwenden:

```env
NEXUSCHAT_MODEL="llama3.2"      # Schnell f√ºr UI
CYPHERMIND_MODEL="llama3.1"     # Gr√∂√üer f√ºr bessere Analyse
CYPHERTRADE_MODEL="mistral"     # Pr√§zise f√ºr Ausf√ºhrung
```

## üß™ Ollama testen

### Test 1: Ollama Server pr√ºfen

```bash
curl http://localhost:11434/api/tags
```

Sollte die installierten Modelle auflisten.

### Test 2: Modell testen

```bash
ollama run llama3.2
```

Interaktiver Chat √∂ffnet sich. Testen Sie das Modell.

### Test 3: API-Kompatibilit√§t testen

```bash
curl http://localhost:11434/v1/models
```

Sollte OpenAI-kompatible API Response zur√ºckgeben.

## üìä Agent-Konfiguration anpassen

Die Agent-Prompts k√∂nnen ohne Code-Update angepasst werden:

**Dateien:** `/app/backend/agent_configs/*.yaml`

### Beispiel: Modell f√ºr CypherMind √§ndern

1. √ñffne `/app/backend/.env`
2. √Ñndere: `CYPHERMIND_MODEL="llama3.1"`
3. Restart: `sudo supervisorctl restart backend`

### Beispiel: Prompt f√ºr CypherMind anpassen

1. √ñffne `/app/backend/agent_configs/cyphermind_config.yaml`
2. Bearbeite das `system_message` Feld
3. Restart: `sudo supervisorctl restart backend`

## üîß Performance-Optimierung

### GPU-Beschleunigung (NVIDIA)

Ollama nutzt automatisch verf√ºgbare GPUs. Pr√ºfen mit:

```bash
ollama ps
```

### Mehrere Modelle parallel

Ollama l√§dt Modelle dynamisch. Sie k√∂nnen mehrere Modelle gleichzeitig verwenden:

```env
NEXUSCHAT_MODEL="llama3.2"
CYPHERMIND_MODEL="llama3.1"
CYPHERTRADE_MODEL="mistral"
```

### RAM-Management

Ollama entl√§dt ungenutzte Modelle automatisch nach 5 Minuten.

Manuell entladen:
```bash
ollama stop llama3.2
```

## üö® Troubleshooting

### Problem: Ollama Server nicht erreichbar

**L√∂sung:**
```bash
# Server starten
ollama serve

# Port pr√ºfen
netstat -tlnp | grep 11434
```

### Problem: Modell nicht gefunden

**L√∂sung:**
```bash
# Verf√ºgbare Modelle auflisten
ollama list

# Modell herunterladen
ollama pull llama3.2
```

### Problem: Langsame Responses

**L√∂sungen:**
1. Kleineres Modell verwenden (z.B. `gemma2` statt `llama3.1`)
2. `temperature` in YAML-Config reduzieren
3. GPU aktivieren (falls verf√ºgbar)

### Problem: Agent-Initialisierung schl√§gt fehl

**Pr√ºfen:**
```bash
# Backend Logs
tail -f /var/log/supervisor/backend.err.log

# Ollama Logs
journalctl -u ollama -f
```

## üìà Modell-Empfehlungen pro Agent

### NexusChat (User Interface)
- **Empfohlen:** `llama3.2` oder `gemma2`
- **Warum:** Schnell, freundlich, gute Sprachqualit√§t
- **Temperature:** 0.7

### CypherMind (Decision & Strategy)
- **Empfohlen:** `llama3.1` oder `mistral`
- **Warum:** Besseres Reasoning, strukturierte Ausgaben
- **Temperature:** 0.5 (niedrig f√ºr Konsistenz)

### CypherTrade (Trade Execution)
- **Empfohlen:** `mistral` oder `deepseek-coder`
- **Warum:** Pr√§zise, zuverl√§ssig, gute JSON-Ausgaben
- **Temperature:** 0.3 (sehr niedrig f√ºr Determinismus)

## üîÑ Von OpenAI zu Ollama wechseln

Falls Sie von OpenAI wechseln m√∂chten:

1. **Ollama installieren und Modell pullen**
2. **`.env` anpassen:**
   ```env
   # Alt (OpenAI)
   NEXUSCHAT_API_KEY="sk-..."
   NEXUSCHAT_MODEL="gpt-4"
   
   # Neu (Ollama)
   NEXUSCHAT_BASE_URL="http://localhost:11434/v1"
   NEXUSCHAT_MODEL="llama3.2"
   ```
3. **Backend neu starten**
4. **Testen**

## üí° Vorteile von Ollama

- ‚úÖ **Kostenlos** - Keine API-Kosten
- ‚úÖ **Privat** - Daten bleiben lokal
- ‚úÖ **Schnell** - Keine Netzwerk-Latenz
- ‚úÖ **Offline** - Funktioniert ohne Internet
- ‚úÖ **Flexibel** - Viele Modelle zur Auswahl

## ‚ö†Ô∏è Limitierungen

- ‚ùå Ben√∂tigt lokale Rechenleistung (CPU/GPU)
- ‚ùå Modelle k√∂nnen 4-8 GB RAM nutzen
- ‚ùå Qualit√§t kann je nach Modell variieren
- ‚ùå Kleinere Modelle weniger "intelligent" als GPT-4

## üîó Ressourcen

- **Ollama Website:** https://ollama.com
- **Modell-Bibliothek:** https://ollama.com/library
- **GitHub:** https://github.com/ollama/ollama
- **Discord:** https://discord.gg/ollama

## üìù N√§chste Schritte

1. ‚úÖ Ollama installiert
2. ‚úÖ Modell heruntergeladen
3. ‚úÖ Backend konfiguriert
4. ‚úÖ Backend neu gestartet
5. ‚úÖ Dashboard √∂ffnen und Bot testen!

---

**Tipp:** Starten Sie mit `llama3.2` f√ºr alle Agents und optimieren Sie sp√§ter je nach Bedarf!
